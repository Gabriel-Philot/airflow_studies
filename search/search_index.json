{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Documentation Airflow-Docker Studies</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>Intro</code> - What is Airflow? \u2705\ud83d\udd28</li> <li><code>v0</code> - First steps and examples. \u2705</li> <li><code>v1</code> - v0 spotify etl example with Medalion Architeture simulation. \u2705</li> <li><code>v2</code> - v1 + also persisting the data on a db. \u2705</li> <li><code>v3</code> - v2 + other datasources (webscraping, api, etc). \ud83d\udca1</li> <li><code>v4</code> - v3 + creating a new operator \ud83d\udca1</li> <li><code>v5</code> - Workings with alerts and logging. \ud83d\udca1</li> <li><code>v6</code> - Docker Operator (Spark?).\ud83d\udca1</li> <li><code>v7</code> - DBT \ud83d\udca1</li> <li><code>v8</code> - AIRBYTE \ud83d\udca1</li> <li><code>v9</code> - K8 \ud83d\udca1</li> </ul>"},{"location":"#link-for-the-repo-httpsgithubcomgabriel-philotairflow_studies","title":"Link for the repo: https://github.com/Gabriel-Philot/airflow_studies","text":""},{"location":"intro/","title":"What is Airflow ?","text":"<p>The Apache Airflow is an open-source data workflow and orchestration platform founded in 2014 by Airbnb.</p> <p>Airbnb's initiative to develop Apache Airflow reflects the increasing complexity and demand for efficiency in data management. Just as a symphony requires a conductor for each instrument and section, data pipelines require scheduling, monitoring, and precise coordination. Without this orchestration, data processes can conflict, leading to delays, data loss, or catastrophic failures. </p> <p>Understanding the context behind Airflow's necessity allows readers to better appreciate its importance and impact. Just as an audience eagerly awaits the conductor's entrance before a performance, we can anticipate the moment when Apache Airflow takes the stage, bringing order and efficiency to the chaos of data workflows.</p> <p>Imagine Apache Airflow as a skilled conductor leading an orchestra of data. Just as a conductor coordinates different instruments to create musical harmony, Airflow orchestrates various data processes into a cohesive workflow.</p> <p>At the beginning of the data concert, Airflow's conductor calls the right musicians to the stage, represented by data extraction tasks fetching necessary information from diverse sources. Like a vigilant conductor, Airflow ensures each \"instrument\" (or task) starts at the right moment, establishing dependencies between them to ensure the workflow progresses smoothly and efficiently.</p> <p>As the concert progresses, Airflow's conductor closely monitors the progress, ensuring each note is played at the right time. It maintains the rhythm of the data flow, scheduling tasks to execute at the most appropriate moments, ensuring the data process is completed on time and accurately.</p> <p>Similar to a conductor leading a memorable performance, Apache Airflow guides data processes with reliability, scalability, and extensibility. It empowers users to create complex data symphonies, allowing them to shape the workflow according to their needs and monitor progress in real-time. With Airflow, the data stage transforms into a spectacle of efficiency and control.</p> <p>Let's delve into the key structures of Apache Airflow and how they work together seamlessly to orchestrate data workflows.</p> <p>\u27a1\ufe0f DAGs (Directed Acyclic Graphs) serve as the backbone of Airflow, akin to musical scores organizing the notes of a piece. Within Airflow, they represent workflows as directed graphs, where each node denotes a task and edges indicate dependencies between tasks. Just as in music, each note (or task) is crucial for the overall harmony.</p> <p>\ud83d\udc77 Operators function as the performers executing tasks outlined in DAGs. They define what needs to be done at each stage of the workflow, such as executing SQL, transferring data, or sending emails. Each operator contributes its unique skill to the overall performance.</p> <p>\ud83d\udcc6 Schedulers and Executors play the roles of conductors and the orchestra, ensuring tasks are executed at the right time and efficiently. The scheduler schedules task execution based on dependencies and specified timings, while the executor carries out the scheduled tasks, whether locally, in clusters, or in the cloud.</p> <p>\ud83c\udf10 Plugins act as special additions to Airflow's repertoire, enhancing its capabilities by adding new operators, connections to external systems, or customizing the user interface. Plugins enrich the user experience, providing more options and flexibility.</p> <p>\ud83d\udcbe Metadata and Databases function as the concert's record and score sheets. They store crucial information about DAGs, tasks, and executions, enabling state control, monitoring, and execution history logging. These metadata form the foundation for ensuring data operations' consistency and reliability.</p> <p>By integrating these structures within Airflow's technological context, we can visualize how it elegantly coordinates data processes, creating a symphony of efficiency and control. Each element plays a vital role, contributing to the smooth and successful execution of data workflows.</p> <p>\ud83d\udcbb Note: Probably will keep enhancing and adding/changing things here.</p>"},{"location":"p2-v0/","title":"v0","text":"<p>Ref:</p>"},{"location":"p2-v0/#link-airflow-docs","title":"Link airflow-docs","text":""},{"location":"p2-v0/#link-docker-docs","title":"Link docker-docs","text":""},{"location":"p2-v0/#my-own-docker-studies-repo","title":"My own Docker-studies repo","text":""},{"location":"p2-v0/#friend-repos-refence-for-starting-out-balemar-bernardo-alemar","title":"Friend repo's refence for starting out (bAlemar Bernardo Alemar)","text":""},{"location":"p2-v0/#youtube-airflow-ref-video","title":"youtube airflow ref video","text":""},{"location":"p2-v0/#dev-good-habits-ref-video-luhborba-luciano-borba","title":"dev good habits ref video (luhborba Luciano Borba)","text":""},{"location":"p2-v0/#youtube-airflow-ref-video-pass-data-betweenn-tasks","title":"youtube airflow ref video (pass data betweenn tasks)","text":""},{"location":"p2-v0/#first-steps-to-start-the-airflow_extended-image-and-some-dag-examples","title":"First steps to start the airflow_extended image and some DAG examples.","text":"<p>Here, our goal will be to make some easy DAGs to get started. One thing that's different from other first-step guides is that this version (v0) starts with the use of imported functions from DAG 0 and beyond.</p> <p>Also, it's important to mention that because this approach will be using my own functions, it will utilize only PythonOperator.</p> <ul> <li>The first thing after installing all the requirements is to create the Dockerfile and docker-compose.yaml files.</li> </ul> <p>[!Note] The project's organization will affect the interaction between these two files.</p>"},{"location":"p2-v0/#important-points-to-be-awere-here","title":"Important points to be awere here:","text":"<p>In docker-compose.file:</p> <p>(1) Look for the image part of the aiflow: <code>image: ${AIRFLOW_IMAGE_NAME:-{image name}:{image tag}}</code></p> <p>e.g.: <code>image: ${AIRFLOW_IMAGE_NAME:-extening_v0_airflow_image:1.0.0}</code></p> <p>this image name and image tag wil be used in the docker build command.</p> <p>(2) in volumes add your src folder, to get autoreload of your modules.</p> <pre><code>volumes:\n    ....\n    ....\n    ....\n    - ./src:/usr/local/airflow/src # (!!!!)\n</code></pre> <p>(3) [Optional] here turn off the tons of example dags that floods our vision. <code>AIRFLOW__CORE__LOAD_EXAMPLES: 'false</code></p>"},{"location":"p2-v0/#intructions-to-build-and-run-the-airflow-docker-image","title":"Intructions to build and run the airflow-docker image:","text":"<p>(1) Build the Dockerfile (extended image):</p> <p><code>docker build . --tag {image name}:{image tag}</code> SAME AS IN docker-compose.file</p> <p>(2) Giving linux some variables definitions:</p> <p><code>echo -e \"AIRFLOW_UID=$(id -u)\\nAIRFLOW_GID=0\" &gt; .env</code></p> <p>(3) Run the docker-compose first start for airflow</p> <p><code>docker-compose up airflow-init</code></p> <p>Here dont worry, after this its done problably you will see your airflow-container going down by him self.</p> <pre><code>airflow-init_1       |airflow already exist in the db\nairflow-init_1       | 2.8.2\nv0_airflow-init_1 exited with code 0\n</code></pre> <p>Don't worry, these are normal checks to see if the database is already created.</p> <p>(4) Normal docker-compose up to start the airflow and start the game</p> <p><code>docker-compose up</code></p>"},{"location":"p2-v0/#airflow-web-httplocalhost8080","title":"Airflow-web: http://localhost:8080/","text":"<ul> <li>username: airflow</li> <li>password: airflow</li> </ul> <p>If you're not familiar with the Airflow UI, I strongly recommend taking some more introductory courses. However, I will mainly focus on the DAGs, specifically how to manually trigger them and examine the logs of the runs.</p> <p>But here its a visual example of how to navigate the UI.</p> <p></p>"},{"location":"p2-v0/#dags","title":"DAGS:","text":""},{"location":"p2-v0/#first-dag-dag_import_testpy","title":"First dag: dag_import_test.py","text":"<p>This dag its just a test to see if we can import modules from other files. it uses two functions.</p> Source code in <code>v0/src/resources/test_module.py</code> <pre><code>def get_requests():\n    import requests\n\n    print(f\"Requestes: {requests.__version__}\")\n</code></pre> Source code in <code>v0/src/resources/test_module.py</code> <pre><code>def get_beatiful():\n    import bs4\n\n    print(f\"bs4: {bs4.__version__}\")\n</code></pre> <p>(Just two functions that basically print a library.)</p>"},{"location":"p2-v0/#second-dag-dag_refresh_testpy","title":"Second dag: dag_refresh_test.py","text":"<p>This dag its just a test to see if the modules can be update without the need to restart the container.</p>"},{"location":"p2-v0/#third-dag_spotify_testpy","title":"Third: dag_spotify_test.py","text":"<p>This dag its a short adapitation of one docker studie that i done, my goal was to test out the extraction and process functions in airflow.</p> Source code in <code>v0/src/main.py</code> <pre><code>def extract_api(artist_name):\n    spotifyApi = SpotifyAPI(client_id, client_secret)\n    artist = spotifyApi.search_for_artist(artist_name)\n    if artist:\n        songs = spotifyApi.get_songs_by_artist(artist[\"id\"], \"BR\")\n        print(f\"Songs found: {len(songs)}\")\n        return artist, songs\n\n    else:\n        print(f\"No artist found with name {artist_name}\")\n        return None\n</code></pre>"},{"location":"p2-v0/#notice-that-in-the-airflow-context-obtaining-the-return-of-a-different-task-has-its-own-mechanism","title":"Notice that in the Airflow context, obtaining the return of a different task has its own mechanism.","text":"Source code in <code>v0/src/main.py</code> <pre><code>def transform_api(**kwargs):\n    ti = kwargs[\"ti\"]\n    artist, songs = ti.xcom_pull(task_ids=\"extract_data\")\n    top10 = transform_json_top10(artist, songs)\n    top10_df = transform_types(top10)\n    print(top10_df.head())\n    return top10_df\n</code></pre> <p>so basically its a excract, load and print dag for testing airflow+docker working togetter.</p>"},{"location":"p2-v0/#list-of-others-useful-docker-commands-here","title":"LIST OF OTHERS USEFUL DOCKER-COMMANDS HERE:","text":"<pre><code>docker-compose down\ndocker ps\ndocker ps -a\ndocker images\ndocker image rm {image.name (or id)}:{image.tag} # Correct way to erase your images\ndocker rmi $(docker images -q) # ERASES ALL THE IMAGES !!! NOT RECOMMENDED TO USE NEAR PROD SYSTEMS\ndocker volume ls\ndocker volume rm {volume.name}\n\n</code></pre> <p>\ud83c\udf40 TIP: # use autocomplete tab(botton of your keyboard) to names or ids</p>"},{"location":"p3-v1/","title":"v1","text":"<p>Ref:</p>"},{"location":"p3-v1/#local-data-airflow-ref","title":"Local data airflow ref","text":""},{"location":"p3-v1/#what-is-a-medallion-architecture-databricks","title":"What is a Medallion Architecture? (Databricks)","text":""},{"location":"p3-v1/#simulating-a-medallion-architecture-with-etl-orchestrated-by-airflow","title":"Simulating a \"Medallion\" architecture with ETL orchestrated by Airflow","text":"<p>\u2757WARNING: Airflow isnt design to be a process data tool \ud83d\udd27, but to orchestrat the diferent tools to process data and manage the process. But in this exemple and sometimes it can do it with small amounts of data.</p> <p>\u2757WARNING: The term \"simulation of Medallion architecture\" is used because the data will be stored outside of a data lake (locally). I chose to begin with this step for the sake of a simpler and \ud83d\udcb0 cost-free \ud83d\udcb0 example.</p> <p>I'm continuing to utilize my own functions and restricting the use to PythonOperator exclusively for this version</p>"},{"location":"p3-v1/#building-upon-v0-as-the-base-model-with-almost-identical-files-and-structures-we-only-need-to-concern-ourselves-with-minor-changes","title":"Building upon V0 as the base model (with almost identical files and structures), we only need to concern ourselves with minor changes.","text":"<p>(The focus here is to highlight the significant differences and the challenging aspects that require attention and effort to resolve.)</p>"},{"location":"p3-v1/#important-points-to-be-awere-here","title":"Important points to be awere here:","text":"<p>In docker-compose.file:</p> <p>(only highlight the changes)</p> <p>(1) in volumes add your src &amp; datalake folder, to get autoreload of your modules.</p> <pre><code>volumes:\n    ....\n    ....\n    ....\n    - ./src:/usr/local/airflow/src # (!!!!)\n    - ./datalake:/opt/local/airflow/datalake # (!!!!)\n</code></pre> <p>\u2757WARNING: Here you need to pay very atetention cause this is folder that later wil be acessed by the airflow-container. Its diferent from the src folder cause you need to write/create things in there.</p> <p>The path of this folder can be a bit tricky to debug, as it varies depending on the operating system. However, since we're running it in a container, we must ensure that the path matches the one inside the container.</p> <p>One exemple here for the sake of explain the situation:</p> <pre><code>\n# HIGHLIGHT on the datadatalake_path = volume in docker-compose.file\ndatalake_path = '/opt/local/airflow/datalake'\nbronze_path = f\"{data_path}/bronze\"\n\ndef json_save_file_bronze(json_data):\n    \"\"\"\n    This function creates a json file in the raw folder.\n    \"\"\"\n    path_datalake_zone = bronze_path\n    file_name = file_name_bronze\n    print(f\"---------- Saving file: {path_datalake_zone}/{file_name}\")\n    # Creates folder if doesnt exist\n    os.makedirs(path_datalake_zone, exist_ok=True)\n    dest_path = f\"{bronze_path}/{file_name}.json\"\n\n    with open(dest_path, 'w', encoding='utf-8') as json_file:\n        # save file in dest path\n        json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n    print(f\"\\nFile created at: {path_datalake_zone}\")\n\n</code></pre> <p>(2) For the sake of fixing the content.</p> <p>look for image: <code>${AIRFLOW_IMAGE_NAME:-{image name}:{image tag}}</code> in compose file</p> <p>this time i changed v0 -&gt; v1 <code>image: ${AIRFLOW_IMAGE_NAME:-extening_v1_airflow_image:1.0.0}</code></p> <p>this image name and image tag wil be used in the docker build command. <code>docker build . --tag extening_v1_airflow_image:1.0.0</code></p>"},{"location":"p3-v1/#other-files","title":"Other files:","text":"<p>Did some changes in the functions, and add a feel others to adequate our goal here. </p> <p>The extraction part know saves the data in json format in the bronze zone </p> Source code in <code>v1/src/ingestion_job.py</code> <pre><code>def extract_api():\n    spotifyApi = SpotifyAPI(client_id, client_secret)\n    artist = spotifyApi.search_for_artist(artist_name)\n    if artist:\n        songs = spotifyApi.get_songs_by_artist(artist, \"BR\")\n        print(f\"Songs found: {len(songs)}\")\n        json_save_file_bronze(songs)\n\n    else:\n        print(f\"No artist found with name {artist_name}\")\n        return None\n</code></pre> <p>The transform_json_top_10 wil now be receaving a json file, and the step is calling other function that saves the data in parquet format in the silver zone </p> Source code in <code>v1/src/processing_job.py</code> <pre><code>def transform_api():\n    with open(bronze_file_path, encoding=\"utf-8\") as f:\n        songs = json.load(f)\n    top10 = transform_json_top10(songs)\n    top10_df = transform_types(top10)\n    print(top10_df.head())\n    save_parquet_from_json(top10_df)\n</code></pre> <p>\ud83d\udd0e Take a look and explore other changes in the code, guess there's no great changes or additions that i forgot to highlight here.</p> <p>Utilize the same instructions from v0 (with the necessary modifications) to start Airflow.</p>"},{"location":"p3-v1/#dags","title":"DAGS:","text":""},{"location":"p3-v1/#spotify_test01","title":"spotify_test01:","text":"<p>[Resume] This DAG will extract data from the Spotify API and save it in the bronze zone (raw JSON format). Then, it will transform the data and save it in the silver zone (Parquet format). Finally, it will apply business rules to the data and save it in the gold zone (CSV format).</p> <p>For each task, there's a main function designed to solve each part, which will be called upon execution.</p>"},{"location":"p3-v1/#extract_task","title":"extract_task","text":"Source code in <code>v1/src/ingestion_job.py</code> <pre><code>def extract_api():\n    spotifyApi = SpotifyAPI(client_id, client_secret)\n    artist = spotifyApi.search_for_artist(artist_name)\n    if artist:\n        songs = spotifyApi.get_songs_by_artist(artist, \"BR\")\n        print(f\"Songs found: {len(songs)}\")\n        json_save_file_bronze(songs)\n\n    else:\n        print(f\"No artist found with name {artist_name}\")\n        return None\n</code></pre>"},{"location":"p3-v1/#process_task","title":"process_task","text":"Source code in <code>v1/src/processing_job.py</code> <pre><code>def transform_api():\n    with open(bronze_file_path, encoding=\"utf-8\") as f:\n        songs = json.load(f)\n    top10 = transform_json_top10(songs)\n    top10_df = transform_types(top10)\n    print(top10_df.head())\n    save_parquet_from_json(top10_df)\n</code></pre>"},{"location":"p3-v1/#transform_task","title":"transform_task","text":"Source code in <code>v1/src/businessrules_job.py</code> <pre><code>def trasform_silver_data():\n    df = pd.read_parquet(silver_file_path)\n    grupocols = [\"name_artist\", \"album_name\"]\n    df_agregate = (\n        df.groupby(grupocols).size().reset_index().rename(columns={0: \"kpi_songs\"})\n    )\n    save_gold_data(df_agregate)\n</code></pre>"},{"location":"p4-v2/","title":"v2","text":"<p>Ref:</p>"},{"location":"p4-v2/#open-workshop-from-luciano-vasconcelos-filho","title":"Open workshop from Luciano Vasconcelos Filho","text":""},{"location":"p4-v2/#local-data-airflow-ref","title":"Local data airflow ref","text":""},{"location":"p4-v2/#v1-persisting-the-data-on-a-cloud-render-database","title":"v1 + Persisting the data on a cloud (Render) database.","text":"<p>\ud83d\udcbb Note: The downstream part will not be touched in this version. The illustration in the image was only to show that after this stage, the data will be ready for consumption.</p> <p>\ud83d\udcbb Note: Usually, Medallion arcs don't need to persist data in a database, the goal was exercise external conection with a cloud e.g. Render.</p> <p>\u2757WARNING: Airflow isnt design to be a process data tool \ud83d\udd27, but to orchestrat the diferent tools to process data and manage the process. But in this exemple and sometimes it can do it with small amounts of data.</p> <p>\u2757WARNING: The term \"simulation of Medallion architecture\" is used because the data will be stored outside of a data lake (locally). I chose to begin with this step for the sake of a simpler and \ud83d\udcb0 cost-free \ud83d\udcb0 example.</p> <p>\ud83c\udf40Tip: The choice of Render as a cloud was made for the sake of a simpler and \ud83d\udcb0 cost-free \ud83d\udcb0 example.</p>"},{"location":"p4-v2/#building-upon-v1-as-the-base-model-with-almost-identical-files-and-structures-we-only-need-to-concern-ourselves-with-minor-changes","title":"Building upon V1 as the base model (with almost identical files and structures), we only need to concern ourselves with minor changes.","text":"<p>(The focus here is to highlight the significant differences and the challenging aspects that require attention and effort to resolve.)</p>"},{"location":"p4-v2/#important-points-to-be-awere-here","title":"Important points to be awere here:","text":"<p>(1) Same as in the previous version, to be careful about how you mount your data in docker-compose.file (check v1.)</p> <p>(2) For the sake of fixing the content.</p> <p>look for image: <code>${AIRFLOW_IMAGE_NAME:-{image name}:{image tag}}</code> in compose file</p> <p>this time i changed v0 -&gt; v1 <code>image: ${AIRFLOW_IMAGE_NAME:-extening_v2_airflow_image:1.0.0}</code></p> <p>this image name and image tag wil be used in the docker build command. <code>docker build . --tag extening_v2_airflow_image:1.0.0</code></p>"},{"location":"p4-v2/#other-files","title":"Other files:","text":"<p>Add the files and dev the code that were necessary to load the data into the cloud PostegresSQL database. (here i did somechanges from my only docker-version of postegress modules.)</p> <p>Silver zone -&gt; Silver table </p> Source code in <code>v2/src/load_db_cloud_job.py</code> <pre><code>def save_db_postgress_silver():\n    df = pd.read_parquet(silver_file_path)\n    db = PostgresDb(host_postegress, db_postegress, user_postegress, pass_postegress, port)\n    db.connect()\n    db.create_update_table(df, schema_name_airflow, table_name_silver, columns_schema_silver, key_column_silver)\n</code></pre> <p>Golden zone -&gt; Golden table </p> Source code in <code>v2/src/load_db_cloud_job.py</code> <pre><code>def save_db_postgress_golden():\n    df = pd.read_csv(gold_file_path)\n    df[\"key_test\"] = df[\"name_artist\"] + df[\"album_name\"] # Adjusting wil be done here sometimes\n    db = PostgresDb(host_postegress, db_postegress, user_postegress, pass_postegress, port)\n    db.connect()\n    db.create_update_table(df, schema_name_airflow, table_name_gold, columns_schema_gold, key_column_gold)\n</code></pre> <p>Have some changes in the dags files but wil touch it when it come to the topic.</p> <p>Utilize the same instructions from v0 (with the necessary modifications) to start Airflow.</p> <p></p>"},{"location":"p4-v2/#v2_dag","title":"v2_dag:","text":"<p>\ud83d\udcbb Note: The first thing to notice will be the groups, but I've only included them for the sake of the exercise; they have no utility or meaning, so you can just skip them.</p> <p>\ud83d\udcbb Note: I've already displayed the tables in the cloud database for the sake of completion.</p> <p>[Resume] This DAG will do exactly the same thing as v1 and create two tables in the cloud database.</p>"},{"location":"p4-v2/#extract_task","title":"extract_task","text":"Source code in <code>v1/src/ingestion_job.py</code> <pre><code>def extract_api():\n    spotifyApi = SpotifyAPI(client_id, client_secret)\n    artist = spotifyApi.search_for_artist(artist_name)\n    if artist:\n        songs = spotifyApi.get_songs_by_artist(artist, \"BR\")\n        print(f\"Songs found: {len(songs)}\")\n        json_save_file_bronze(songs)\n\n    else:\n        print(f\"No artist found with name {artist_name}\")\n        return None\n</code></pre>"},{"location":"p4-v2/#process_task","title":"process_task","text":"Source code in <code>v1/src/processing_job.py</code> <pre><code>def transform_api():\n    with open(bronze_file_path, encoding=\"utf-8\") as f:\n        songs = json.load(f)\n    top10 = transform_json_top10(songs)\n    top10_df = transform_types(top10)\n    print(top10_df.head())\n    save_parquet_from_json(top10_df)\n</code></pre>"},{"location":"p4-v2/#transform_task","title":"transform_task","text":"Source code in <code>v1/src/businessrules_job.py</code> <pre><code>def trasform_silver_data():\n    df = pd.read_parquet(silver_file_path)\n    grupocols = [\"name_artist\", \"album_name\"]\n    df_agregate = (\n        df.groupby(grupocols).size().reset_index().rename(columns={0: \"kpi_songs\"})\n    )\n    save_gold_data(df_agregate)\n</code></pre>"},{"location":"p4-v2/#task-group-silver-table","title":"task group Silver table","text":"Source code in <code>v2/src/load_db_cloud_job.py</code> <pre><code>def save_db_postgress_silver():\n    df = pd.read_parquet(silver_file_path)\n    db = PostgresDb(host_postegress, db_postegress, user_postegress, pass_postegress, port)\n    db.connect()\n    db.create_update_table(df, schema_name_airflow, table_name_silver, columns_schema_silver, key_column_silver)\n</code></pre>"},{"location":"p4-v2/#task-group-golden-table","title":"task group Golden table","text":"Source code in <code>v2/src/load_db_cloud_job.py</code> <pre><code>def save_db_postgress_golden():\n    df = pd.read_csv(gold_file_path)\n    df[\"key_test\"] = df[\"name_artist\"] + df[\"album_name\"] # Adjusting wil be done here sometimes\n    db = PostgresDb(host_postegress, db_postegress, user_postegress, pass_postegress, port)\n    db.connect()\n    db.create_update_table(df, schema_name_airflow, table_name_gold, columns_schema_gold, key_column_gold)\n</code></pre> <p>\ud83d\udcbb Note: In Airflow, you can usually use different operators that have specific usage. For example, you can use the PostgresOperator to make your life easier. However, sometimes when you need to change the database or use a different kind of solution, it's nice to know how to build it manually. Also, the solution for it was very near my Docker studies. I had to make some adaptations, but it was very helpful for the base. So, knowing the basics of how this process of extract, transform, and load works gives more freedom and insights through the solutions.</p> <p>Here must be referenced Luciano Vasconcelos Filho for this insight of using your own built-in solutions to grasp the fundamentals of the process, which gives you a better view of how the tool works and prevents you from being stuck with new kinds of solutions.</p>"}]}